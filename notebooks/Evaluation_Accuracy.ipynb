{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook: Build evaluation method\n",
        "* Aim at >.90 accuracy\n",
        "\n",
        "currently it is tested with yolov5 prediction results.\n",
        "\n",
        "It is compatible for all torch prediction outputs in the form of .pandas().xywh (xcenter, ycenter, width, height).\n",
        "\n",
        "Using Google Colab to view this notebook is highly recommended.\n"
      ],
      "metadata": {
        "id": "qN8V989HzHDT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D7J191IYwp8",
        "outputId": "015e9ea9-6b3f-4774-e798-07866d3a07e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Oct 14 16:04:14 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    45W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Prep works, install yolov5, download and partition datasets"
      ],
      "metadata": {
        "id": "_9jgRJYlL_-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TTOdqkAk_Ad"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/ultralytics/yolov5 \n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt #wandb\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBHHieZsCbGS"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ExifTags\n",
        "from pycocotools.coco import COCO\n",
        "from matplotlib.patches import Polygon, Rectangle\n",
        "from matplotlib.collections import PatchCollection\n",
        "import colorsys\n",
        "import random\n",
        "import pylab\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "from tqdm import tqdm\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import re\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mount_drive = False\n",
        "if mount_drive:\n",
        "  # gdown a gdrive file too frequently triggers google's control and makes the file un-gdown-able\n",
        "  # in this case, go to 1hq0KcSM31yrR4YlWqM_P29Y3YTuvuIom and 1X3O2v3GIPveq3ylWF6o1qHI5uzbN1vWA, manually\n",
        "  # make a copy of them to your own drive and mount your drive to the colab instance, then you can manipulate freely\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  %cp ./drive/MyDrive/rotated2.zip ./\n",
        "  %cp /content/drive/MyDrive/trash_ai_trained_weights/yolov5x6_best_weights.pt ./\n",
        "  else:\n",
        "    !gdown 1hq0KcSM31yrR4YlWqM_P29Y3YTuvuIom # download best trained yolov5x6 weights\n",
        "    !gdown 1X3O2v3GIPveq3ylWF6o1qHI5uzbN1vWA # download organized TACO images (TACO itself, 1500 images, without unofficial images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0nSxY0wdrr2",
        "outputId": "9d0e06a0-2937-4d01-8a08-54a4361091e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq ./rotated2.zip \n",
        "%mv ./content/* ./"
      ],
      "metadata": {
        "id": "Pfvddv2pS_ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/pedropro/TACO/master/data/annotations.json\n",
        "!wget https://raw.githubusercontent.com/pedropro/TACO/master/data/annotations_unofficial.json"
      ],
      "metadata": {
        "id": "nbwKD15fKpfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPFCzX31IGBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8635bf93-000c-46ac-e30d-3fdd0688584a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of all images:\n",
            "1500\n"
          ]
        }
      ],
      "source": [
        "nr_imgs=None\n",
        "for root, dirnames, filenames in os.walk('./yoloTACO/labels/'):\n",
        "  nr_imgs = len(filenames)\n",
        "  break\n",
        "print('Number of all images:\\n'+str(nr_imgs))\n",
        "\n",
        "## train test split\n",
        "'''\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "'''\n",
        "np.random.seed(5)\n",
        "id_list=[i for i in range(nr_imgs)]\n",
        "np.random.shuffle(id_list)\n",
        "train_ids = id_list[:1300]\n",
        "val_ids = id_list[1300:1400]\n",
        "test_ids = id_list[1400:]\n",
        "\n",
        "def move_helper(ids, desti):\n",
        "  for id in ids:\n",
        "    img_name = os.path.join( './yoloTACO/images', str(id)+'.jpg' )\n",
        "    lbl_name = os.path.join( './yoloTACO/labels', str(id)+'.txt' )\n",
        "    print(img_name)\n",
        "    if os.path.isfile(img_name):\n",
        "        shutil.copy( img_name, './yoloTACO/images/'+desti)\n",
        "        shutil.copy( lbl_name, './yoloTACO/labels/'+desti)\n",
        "    else :\n",
        "        print('file does not exist', img_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwCWClsrSD4z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!mkdir yoloTACO/images/train\n",
        "!mkdir yoloTACO/images/val\n",
        "!mkdir yoloTACO/images/test\n",
        "!mkdir yoloTACO/labels/train\n",
        "!mkdir yoloTACO/labels/val\n",
        "!mkdir yoloTACO/labels/test\n",
        "move_helper(test_ids,'test')\n",
        "move_helper(train_ids,'train')\n",
        "move_helper(val_ids,'val')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir ./datasets\n",
        "mv yoloTACO datasets/"
      ],
      "metadata": {
        "id": "gHyjxWAW7DF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S96FWSElHY9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evH7jgs5Dnj7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title yml\n",
        "\n",
        "with open('./yolov5/data/yoloTACO.yaml', mode='w') as fp:\n",
        "  lines = '''path: ../datasets/yoloTACO  # dataset root dir\n",
        "train: images/train  # train images \n",
        "val: images/val  # val images \n",
        "test: images/test # test images (optional)\n",
        "\n",
        "# Classes\n",
        "names:\n",
        "  0: Aluminium foil\n",
        "  1: Battery\n",
        "  2: Aluminium blister pack\n",
        "  3: Carded blister pack\n",
        "  4: Other plastic bottle\n",
        "  5: Clear plastic bottle\n",
        "  6: Glass bottle\n",
        "  7: Plastic bottle cap\n",
        "  8: Metal bottle cap\n",
        "  9: Broken glass\n",
        "  10: Food Can\n",
        "  11: Aerosol\n",
        "  12: Drink can\n",
        "  13: Toilet tube\n",
        "  14: Other carton\n",
        "  15: Egg carton\n",
        "  16: Drink carton\n",
        "  17: Corrugated carton\n",
        "  18: Meal carton\n",
        "  19: Pizza box\n",
        "  20: Paper cup\n",
        "  21: Disposable plastic cup\n",
        "  22: Foam cup\n",
        "  23: Glass cup\n",
        "  24: Other plastic cup\n",
        "  25: Food waste\n",
        "  26: Glass jar\n",
        "  27: Plastic lid\n",
        "  28: Metal lid\n",
        "  29: Other plastic\n",
        "  30: Magazine paper\n",
        "  31: Tissues\n",
        "  32: Wrapping paper\n",
        "  33: Normal paper\n",
        "  34: Paper bag\n",
        "  35: Plastified paper bag\n",
        "  36: Plastic film\n",
        "  37: Six pack rings\n",
        "  38: Garbage bag\n",
        "  39: Other plastic wrapper\n",
        "  40: Single-use carrier bag\n",
        "  41: Polypropylene bag\n",
        "  42: Crisp packet\n",
        "  43: Spread tub\n",
        "  44: Tupperware\n",
        "  45: Disposable food container\n",
        "  46: Foam food container\n",
        "  47: Other plastic container\n",
        "  48: Plastic glooves\n",
        "  49: Plastic utensils\n",
        "  50: Pop tab\n",
        "  51: Rope & strings\n",
        "  52: Scrap metal\n",
        "  53: Shoe\n",
        "  54: Squeezable tube\n",
        "  55: Plastic straw\n",
        "  56: Paper straw\n",
        "  57: Styrofoam piece\n",
        "  58: Unlabeled litter\n",
        "  59: Cigarette'''\n",
        "  fp.writelines(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdMrwEgnB9C4",
        "outputId": "1eded40f-2c94-4a74-f0b1-74394d2c2623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "benchmarks.py\t detect.py   models\t       setup.cfg       val.py\n",
            "classify\t export.py   README.md\t       train.py\n",
            "CONTRIBUTING.md  hubconf.py  requirements.txt  tutorial.ipynb\n",
            "data\t\t LICENSE     segment\t       utils\n"
          ]
        }
      ],
      "source": [
        "%cd ./yolov5\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XslsKqRuHpmf",
        "outputId": "cc3a38c5-8c0b-4760-e41a-284eb8d9d8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/yolov5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "67Xa-feZH9q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Evaluate with our best trained weights so far"
      ],
      "metadata": {
        "id": "C37qgWyEMLpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## detect and eval with yolo default scripts"
      ],
      "metadata": {
        "id": "fHmO_QRlN4bQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoLh0BGlXQMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92a300c-f497-48ef-a66a-1380417d1645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/yoloTACO.yaml, weights=['./yolov5x6_best_weights.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 ðŸš€ v6.2-194-g2a19d07 Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (A100-SXM4-40GB, 40536MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 416 layers, 140537980 parameters, 0 gradients, 209.1 GFLOPs\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mScanning '/content/datasets/yoloTACO/labels/test' images and labels...100 found, 0 missing, 0 empty, 0 corrupt: 100% 100/100 [00:00<00:00, 2635.51it/s]\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: /content/datasets/yoloTACO/labels/test.cache\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:08<00:00,  2.16s/it]\n",
            "                   all        100        300     0.0464      0.628       0.11      0.102\n",
            "Speed: 0.2ms pre-process, 5.4ms inference, 2.4ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python val.py --data yoloTACO.yaml --task test --weights ./yolov5x6_best_weights.pt\n",
        "#!python detect.py --weights ./yolov5x6_best_weights.pt --source /content/datasets/yoloTACO/images/test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the default `MAP` is not the \"wanted\" metrics for our project, as our sponsor specifically requested a metrics under the name \"accuracy\" and a target score of >.90."
      ],
      "metadata": {
        "id": "4of5Qufu1Pd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## detect with torch framework manually\n",
        "\n",
        "This is a necessary step to use our accuracy evaluator."
      ],
      "metadata": {
        "id": "5Ay3MOsDN90j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='./yolov5x6_best_weights.pt')  # load our local model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30QWyk7yiMsk",
        "outputId": "b9c644c2-3766-4039-b032-92f3c2aa9b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2022-10-14 Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (A100-SXM4-40GB, 40536MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 416 layers, 140537980 parameters, 0 gradients, 209.1 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test imgs\n",
        "test_dir = '/content/datasets/yoloTACO/images/test/'\n",
        "test_list = [i[2] for i in os.walk(test_dir)][0]\n",
        "test_list = [re.findall(r'\\d+',i)[0] for i in test_list]\n",
        "test_read_img_list = [Image.open(test_dir+str(i)+'.jpg') for i in test_list]\n",
        "# alternatively use cv2: cv2.imread('target_path')[..., ::-1]  # OpenCV image (BGR to RGB)\n"
      ],
      "metadata": {
        "id": "GUjCnveZk2sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "results = model(test_read_img_list) # batch of images\n",
        "pred_pd = results.pandas().xywh "
      ],
      "metadata": {
        "id": "RKr_CupeiMvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget -O data/annotations.json https://raw.githubusercontent.com/pedropro/TACO/master/data/annotations.json\n",
        "anno_path = './data/annotations.json'\n",
        "annos = COCO(annotation_file=anno_path)\n",
        "with open(anno_path, 'r') as f:\n",
        "    annos_json = json.loads(f.read())\n",
        "no_to_clname = {i:j for i,j in enumerate([i['name'] for i in annos_json['categories']])}\n"
      ],
      "metadata": {
        "id": "QRj9_Dk_iMxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "truth = [annos.loadAnns(annos.getAnnIds(int(i))) for i in test_list]\n",
        "truth_pd = []\n",
        "for i in truth:\n",
        "  cache = [j['bbox']+[1]+[j['category_id']]+[no_to_clname[j['category_id']]] for j in i]\n",
        "  df = pd.DataFrame(cache,columns = ['xcenter','ycenter','width','height','confidence','class','name'])\n",
        "  df['xcenter'] = df['xcenter'] + df['width']/2\n",
        "  df['ycenter'] = df['ycenter'] + df['height']/2\n",
        "  truth_pd.append(df)"
      ],
      "metadata": {
        "id": "zGFRscK6iMzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Accuracy evaluation\n",
        "\n",
        "For each object with a truth bounding box in each image, if there is a prediction bounding box that has an IOU > threshold with that truth bounding box, it is counted as `detected`.\n",
        "\n",
        "For overall model `accuracy`, we count total number of `detected` of all images over total number of `objects` of all images."
      ],
      "metadata": {
        "id": "TyiyfCEAODQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bbox_iou(box1, box2, eps=1e-7):\n",
        "  # CITATION: adapted from YOLOV5 utils, author, cr: ultralytics\n",
        "  # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n",
        "\n",
        "  # Get the coordinates of bounding boxes, transform from xywh to xyxy\n",
        "  (x1, y1, w1, h1), (x2, y2, w2, h2) = box1.chunk(4, 1), box2.chunk(4, 1)\n",
        "  w1_, h1_, w2_, h2_ = w1 / 2, h1 / 2, w2 / 2, h2 / 2\n",
        "  b1_x1, b1_x2, b1_y1, b1_y2 = x1 - w1_, x1 + w1_, y1 - h1_, y1 + h1_\n",
        "  b2_x1, b2_x2, b2_y1, b2_y2 = x2 - w2_, x2 + w2_, y2 - h2_, y2 + h2_\n",
        "\n",
        "\n",
        "  inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
        "          (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
        "  union = w1 * h1 + w2 * h2 - inter + eps\n",
        "  return inter / union  # return IoU\n",
        "  \n",
        "def each_pic(pred_df,truth_df,iou_th):\n",
        "  nr_objs = truth_df.shape[0]\n",
        "  nr_dets = 0\n",
        "  for i in truth_df.iterrows():\n",
        "    tbox_tensor = torch.tensor([i[1].tolist()[:4]])\n",
        "    tlabel = i[1].tolist()[5]\n",
        "    \n",
        "    row_counter=0\n",
        "    for j in pred_df.iterrows():\n",
        "      pbox_tensor = torch.tensor([j[1].tolist()[:4]])\n",
        "      plabel = j[1].tolist()[5]\n",
        "      if bbox_iou(tbox_tensor,pbox_tensor)>iou_th and tlabel==plabel:\n",
        "        nr_dets+=1\n",
        "        pred_df.drop([row_counter]) # drop matched bbox, so one prediction bbox \n",
        "                                    # wont be counted as \"detected\" for two different objects\n",
        "        continue\n",
        "      row_counter+=1\n",
        "  return nr_objs,nr_dets"
      ],
      "metadata": {
        "id": "LkQgO9T5OHzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def acc(pred,truth,iou_th=0.7):\n",
        "  objs,dets=0,0\n",
        "  for i in tqdm(range(len(truth))):\n",
        "    o,d=each_pic(pred_pd[i],truth_pd[i],iou_th)\n",
        "    objs+=o\n",
        "    dets+=d\n",
        "  return np.round(dets/objs,6)\n",
        "\n",
        "accuracy = acc(pred_pd,truth_pd)"
      ],
      "metadata": {
        "id": "p8E9EsAMOH1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b78c1f85-ae98-481a-cd49-4e651676e7b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 190.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Our trained model has an accuracy of: '+str(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdfiVJvhLPMw",
        "outputId": "096c1660-36a0-4c04-b39b-4f91010559aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our trained model has an accuracy of: 0.6\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}