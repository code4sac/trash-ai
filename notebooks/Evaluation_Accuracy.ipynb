{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook: Build evaluation method\n",
        "* Aim at >.90 accuracy\n",
        "\n",
        "Currently it is tested with yolov5 prediction results. But it is compatible for all prediction outputs as long as they are in the form of .pandas().xywh. (see section `1.3` for examples)\n",
        "\n",
        "Using `Google Colab` to view this notebook is highly recommended."
      ],
      "metadata": {
        "id": "qN8V989HzHDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions:\n",
        "* Want the **big TACO**? i.e. the **unofficial TACO** that contains 5,000+ images. The label quality of the big TACO might be poor. I experimented with it and found a dozen errors in labels (annotations already).\n",
        "\n",
        "* **Reduce target classes**? There are 60 categories and 28 super-categories. Currently we predict 60 classes, which is might be too many considering that we only have less than 1500 training images. Should we use the 28 super-categories as classes to be predicted? Or more radically, 5~10 classes of plastic, metal, glass, etc.\n",
        "\n",
        "* Better **Train/Test split**? Currently I do a fully random 1300/100/100 split for train/val/test. This is obviously not the most common choice. Also, the current split is not stratified -- classes(categories)'s distribution in training and testing set will be different which might be a problem! Input are greatly welcomed!"
      ],
      "metadata": {
        "id": "mS4ry1DRFSes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mount_drive = True #mount only if you have weights and TACO images in your drive already"
      ],
      "metadata": {
        "id": "R_Dc3-3xaTwr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0D7J191IYwp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ac78ed-5e50-4905-de63-ce355d72b779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 18 11:55:50 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Prep works, install yolov5, download and partition datasets"
      ],
      "metadata": {
        "id": "_9jgRJYlL_-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucAYkRiCIxFW",
        "outputId": "fb66db3d-538e-4a04-fbba-224565711ab7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "find . \\! -name 'rotated2.zip' -delete"
      ],
      "metadata": {
        "id": "cJDeErmxIuY8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2TTOdqkAk_Ad"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/ultralytics/yolov5 \n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt #wandb\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FBHHieZsCbGS"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ExifTags\n",
        "from pycocotools.coco import COCO\n",
        "from matplotlib.patches import Polygon, Rectangle\n",
        "from matplotlib.collections import PatchCollection\n",
        "import colorsys\n",
        "import random\n",
        "import pylab\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "from tqdm import tqdm\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not mount_drive:\n",
        "  # gdown a gdrive file too frequently triggers google's control and makes the file un-gdown-able\n",
        "  # in this case, go to 1hq0KcSM31yrR4YlWqM_P29Y3YTuvuIom and 1X3O2v3GIPveq3ylWF6o1qHI5uzbN1vWA, manually\n",
        "  # make a copy of them to your own drive and mount your drive to the colab instance, then you can manipulate freely\n",
        " \n",
        "  !gdown 151cUWIawXdRkVPg5M-aFvlKD67_gENGh # download best trained yolov5x6 weights on original classes\n",
        "  !gdown 1X3O2v3GIPveq3ylWF6o1qHI5uzbN1vWA # download organized TACO images (TACO itself, 1500 images, without unofficial images)\n",
        "\n",
        "if mount_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "  %cp /gdrive/MyDrive/best_yolov5/exp/weights/best.pt /content/yolov5x6_best_weights.pt #get trained weights\n",
        "  if not os.path.isfile('/content/rotated2.zip'):\n",
        "    %cp /gdrive/MyDrive/rotated2_og.zip /content/rotated2.zip #get images\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "M0nSxY0wdrr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb85c267-852c-4c11-8bc2-e9e36fc771f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq /content/rotated2.zip \n",
        "%mv /content/content/* /content/"
      ],
      "metadata": {
        "id": "Pfvddv2pS_ZX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/pedropro/TACO/master/data/annotations.json\n",
        "!wget https://raw.githubusercontent.com/pedropro/TACO/master/data/annotations_unofficial.json"
      ],
      "metadata": {
        "id": "nbwKD15fKpfa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CPFCzX31IGBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a0b5ed-d17f-440c-c60b-1cbdbe8a5845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of all images:\n",
            "1500\n"
          ]
        }
      ],
      "source": [
        "nr_imgs=None\n",
        "for root, dirnames, filenames in os.walk('./yoloTACO/labels/'):\n",
        "  nr_imgs = len(filenames)\n",
        "  break\n",
        "print('Number of all images:\\n'+str(nr_imgs))\n",
        "\n",
        "## train test split\n",
        "'''\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "'''\n",
        "np.random.seed(4)\n",
        "id_list=[i for i in range(nr_imgs)]\n",
        "np.random.shuffle(id_list)\n",
        "train_ids = id_list[:1300]\n",
        "val_ids = id_list[1300:1400]\n",
        "test_ids = id_list[1400:]\n",
        "\n",
        "def move_helper(ids, desti):\n",
        "  for id in ids:\n",
        "    img_name = os.path.join( './yoloTACO/images', str(id)+'.jpg' )\n",
        "    lbl_name = os.path.join( './yoloTACO/labels', str(id)+'.txt' )\n",
        "    print(img_name)\n",
        "    if os.path.isfile(img_name):\n",
        "        shutil.copy( img_name, './yoloTACO/images/'+desti)\n",
        "        shutil.copy( lbl_name, './yoloTACO/labels/'+desti)\n",
        "    else :\n",
        "        print('file does not exist', img_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kwCWClsrSD4z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!mkdir yoloTACO/images/train\n",
        "!mkdir yoloTACO/images/val\n",
        "!mkdir yoloTACO/images/test\n",
        "!mkdir yoloTACO/labels/train\n",
        "!mkdir yoloTACO/labels/val\n",
        "!mkdir yoloTACO/labels/test\n",
        "move_helper(test_ids,'test')\n",
        "move_helper(train_ids,'train')\n",
        "move_helper(val_ids,'val')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir ./datasets\n",
        "mv yoloTACO datasets/"
      ],
      "metadata": {
        "id": "gHyjxWAW7DF0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced=False #True if using reduced classes (28 categories)"
      ],
      "metadata": {
        "id": "S96FWSElHY9d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "evH7jgs5Dnj7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title yml\n",
        "\n",
        "if reduced == True:\n",
        "\n",
        "  with open('/content/yolov5/data/yoloTACO.yaml', mode='w') as fp:\n",
        "    lines = '''path: ../datasets/yoloTACO  # dataset root dir\n",
        "train: images/train  # train images \n",
        "val: images/val  # val images \n",
        "test: images/test # test images (optional)\n",
        "\n",
        "# Classes\n",
        "names:\n",
        "  0: Aluminium foil\n",
        "  1: Battery\n",
        "  2: Blister pack\n",
        "  3: Bottle\n",
        "  4: Bottle cap\n",
        "  5: Broken glass\n",
        "  6: Can\n",
        "  7: Carton\n",
        "  8: Cup\n",
        "  9: Food waste\n",
        "  10: Glass jar\n",
        "  11: Lid\n",
        "  12: Other plastic\n",
        "  13: Paper\n",
        "  14: Paper bag\n",
        "  15: Plastic bag & wrapper\n",
        "  16: Plastic container\n",
        "  17: Plastic glooves\n",
        "  18: Plastic utensils\n",
        "  19: Pop tab\n",
        "  20: Rope & strings\n",
        "  21: Scrap metal\n",
        "  22: Shoe\n",
        "  23: Squeezable tube\n",
        "  24: Straw\n",
        "  25: Styrofoam piece\n",
        "  26: Unlabeled litter\n",
        "  27: Cigarette'''\n",
        "    fp.writelines(lines)\n",
        "\n",
        "else: \n",
        "  with open('/content/yolov5/data/yoloTACO.yaml', mode='w') as fp:\n",
        "    lines = '''path: ../datasets/yoloTACO  # dataset root dir\n",
        "train: images/train  # train images (relative to 'path') 128 images\n",
        "val: images/val  # val images (relative to 'path') 128 images\n",
        "test: images/test # test images (optional)\n",
        "\n",
        "# Classes\n",
        "names:\n",
        "  0: Aluminium foil\n",
        "  1: Battery\n",
        "  2: Aluminium blister pack\n",
        "  3: Carded blister pack\n",
        "  4: Other plastic bottle\n",
        "  5: Clear plastic bottle\n",
        "  6: Glass bottle\n",
        "  7: Plastic bottle cap\n",
        "  8: Metal bottle cap\n",
        "  9: Broken glass\n",
        "  10: Food Can\n",
        "  11: Aerosol\n",
        "  12: Drink can\n",
        "  13: Toilet tube\n",
        "  14: Other carton\n",
        "  15: Egg carton\n",
        "  16: Drink carton\n",
        "  17: Corrugated carton\n",
        "  18: Meal carton\n",
        "  19: Pizza box\n",
        "  20: Paper cup\n",
        "  21: Disposable plastic cup\n",
        "  22: Foam cup\n",
        "  23: Glass cup\n",
        "  24: Other plastic cup\n",
        "  25: Food waste\n",
        "  26: Glass jar\n",
        "  27: Plastic lid\n",
        "  28: Metal lid\n",
        "  29: Other plastic\n",
        "  30: Magazine paper\n",
        "  31: Tissues\n",
        "  32: Wrapping paper\n",
        "  33: Normal paper\n",
        "  34: Paper bag\n",
        "  35: Plastified paper bag\n",
        "  36: Plastic film\n",
        "  37: Six pack rings\n",
        "  38: Garbage bag\n",
        "  39: Other plastic wrapper\n",
        "  40: Single-use carrier bag\n",
        "  41: Polypropylene bag\n",
        "  42: Crisp packet\n",
        "  43: Spread tub\n",
        "  44: Tupperware\n",
        "  45: Disposable food container\n",
        "  46: Foam food container\n",
        "  47: Other plastic container\n",
        "  48: Plastic glooves\n",
        "  49: Plastic utensils\n",
        "  50: Pop tab\n",
        "  51: Rope & strings\n",
        "  52: Scrap metal\n",
        "  53: Shoe\n",
        "  54: Squeezable tube\n",
        "  55: Plastic straw\n",
        "  56: Paper straw\n",
        "  57: Styrofoam piece\n",
        "  58: Unlabeled litter\n",
        "  59: Cigarette'''\n",
        "    fp.writelines(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CdMrwEgnB9C4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095613d5-7b36-487b-cb2d-7ac4c6169fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "benchmarks.py\t detect.py   models\t       setup.cfg       val.py\n",
            "classify\t export.py   README.md\t       train.py\n",
            "CONTRIBUTING.md  hubconf.py  requirements.txt  tutorial.ipynb\n",
            "data\t\t LICENSE     segment\t       utils\n"
          ]
        }
      ],
      "source": [
        "%cd ./yolov5\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "XslsKqRuHpmf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "715e77db-276e-4af0-cac6-474031e6e241"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/yolov5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "67Xa-feZH9q5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Evaluate with our best trained weights so far"
      ],
      "metadata": {
        "id": "C37qgWyEMLpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 detect and eval with yolo default scripts"
      ],
      "metadata": {
        "id": "fHmO_QRlN4bQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DoLh0BGlXQMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f77fc3-e7b0-447d-e46f-a8add7ba21f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/yoloTACO.yaml, weights=['/content/yolov5x6_best_weights.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 ðŸš€ v6.2-199-gf1482b0 Python-3.7.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 416 layers, 140537980 parameters, 0 gradients, 209.1 GFLOPs\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mScanning '/content/datasets/yoloTACO/labels/test' images and labels...100 found, 0 missing, 0 empty, 0 corrupt: 100% 100/100 [00:00<00:00, 453.47it/s]\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: /content/datasets/yoloTACO/labels/test.cache\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:23<00:00,  5.77s/it]\n",
            "                   all        100        286       0.44       0.26      0.279       0.23\n",
            "Speed: 2.4ms pre-process, 48.8ms inference, 3.8ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python val.py --data yoloTACO.yaml --task test --weights /content/yolov5x6_best_weights.pt\n",
        "#!python detect.py --weights /content/yolov5x6_best_weights.pt --source /content/datasets/yoloTACO/images/test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the default `MAP` is not the \"wanted\" metrics for our project, as our sponsor specifically requested a metrics under the name \"accuracy\" and a target score of >.90."
      ],
      "metadata": {
        "id": "4of5Qufu1Pd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 detect with torch framework manually\n",
        "\n",
        "This is a necessary step to use our accuracy evaluator."
      ],
      "metadata": {
        "id": "5Ay3MOsDN90j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='/content/yolov5x6_best_weights.pt',force_reload=True)  # load our local model"
      ],
      "metadata": {
        "id": "30QWyk7yiMsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f4f194-8acb-46ea-a65f-7fa598cfabf9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 ðŸš€ 2022-10-18 Python-3.7.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 416 layers, 140537980 parameters, 0 gradients, 209.1 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test imgs\n",
        "test_dir = '/content/datasets/yoloTACO/images/test/'\n",
        "test_list = test_ids # [i[2] for i in os.walk(test_dir)][0] # or alternatively read from files # test_list = [re.findall(r'\\d+',i)[0] for i in test_list]\n",
        "\n",
        "test_read_img_list = [Image.open(test_dir+str(i)+'.jpg') for i in test_list] # alternatively use cv2: cv2.imread('target_path')[..., ::-1]  # OpenCV image (BGR to RGB)"
      ],
      "metadata": {
        "id": "GUjCnveZk2sf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "results = model(test_read_img_list) # batch of images\n",
        "pred_pd = results.pandas().xywh\n",
        "\n",
        "for j,i in enumerate(pred_pd):\n",
        "  i=i.assign(image_id=[test_list[j]]*i.shape[0])\n",
        "  pred_pd[j]=i"
      ],
      "metadata": {
        "id": "RKr_CupeiMvD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clear GPU mem\n",
        "def free_memory(to_delete: list, debug=False):\n",
        "    import gc\n",
        "    import inspect\n",
        "    calling_namespace = inspect.currentframe().f_back\n",
        "    if debug:\n",
        "        print('Before:')\n",
        "        torch.get_less_used_gpu(debug=True)\n",
        "\n",
        "    for _var in to_delete:\n",
        "        calling_namespace.f_locals.pop(_var, None)\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    if debug:\n",
        "        print('After:')\n",
        "        torch.get_less_used_gpu(debug=True)\n",
        "\n",
        "free_memory([model])"
      ],
      "metadata": {
        "id": "HvwZN3vzHB5u"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget -O data/annotations.json https://raw.githubusercontent.com/pedropro/TACO/master/data/annotations.json\n",
        "anno_path = './data/annotations.json'\n",
        "annos = COCO(annotation_file=anno_path)\n",
        "with open(anno_path, 'r') as f:\n",
        "    annos_json = json.loads(f.read())\n",
        "no_to_clname = {i:j for i,j in enumerate([i['name'] for i in annos_json['categories']])}\n"
      ],
      "metadata": {
        "id": "QRj9_Dk_iMxG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "truth_pd = []\n",
        "for i in test_list:\n",
        "  img_info = annos.loadImgs(i)[0]    \n",
        "  img_height = img_info['height']\n",
        "  img_width = img_info['width']\n",
        "\n",
        "  cache = pd.read_csv('/content/datasets/yoloTACO/labels/'+str(i)+'.txt',header=None,\n",
        "                      names = ['class','xcenter','ycenter','width','height'],delimiter=' ')\n",
        "  cache[\"xcenter\"] = img_width * cache[\"xcenter\"]\n",
        "  cache[\"ycenter\"] = img_height * cache[\"ycenter\"]\n",
        "  cache[\"width\"] = img_width * cache[\"width\"]\n",
        "  cache[\"height\"] = img_height * cache[\"height\"]\n",
        "\n",
        "  cache = cache.assign(confidence = [1]*cache.shape[0])\n",
        "  cache = cache.reindex(columns=['xcenter','ycenter','width','height','confidence','class'])\n",
        "  cache = cache.assign(image_id = [i]*cache.shape[0])\n",
        "\n",
        "  # cache = cache.assign(img_width = [width]*cache.shape[0])\n",
        "  # cache = cache.assign(img_height = [height]*cache.shape[0])\n",
        "\n",
        "  truth_pd.append(cache)"
      ],
      "metadata": {
        "id": "o6dfoDEzdV-1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 example prediction and truth"
      ],
      "metadata": {
        "id": "QNrQCtxcBiw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_pd[:2] \n",
        "# predictions for first two images\n",
        "# there will be a list of two dataframes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuvboCQRLUvU",
        "outputId": "7882e242-d32f-48cb-f167-ff6fff0d2f0c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[       xcenter      ycenter       width      height  confidence  class  \\\n",
              " 0  1160.030762  2049.695557  682.435669  740.454224    0.885453     36   \n",
              " \n",
              "            name  image_id  \n",
              " 0  Plastic film        86  ,\n",
              "        xcenter     ycenter       width      height  confidence  class  \\\n",
              " 0  1587.685059  496.165466  143.758911  260.257538    0.824172     29   \n",
              " 1  1053.790283  664.041016   94.993835   78.028870    0.748516     29   \n",
              " 2  1585.460449  496.756775  138.394287  270.051270    0.541024     51   \n",
              " \n",
              "              name  image_id  \n",
              " 0   Other plastic       171  \n",
              " 1   Other plastic       171  \n",
              " 2  Rope & strings       171  ]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_pd[1]"
      ],
      "metadata": {
        "id": "WFbRwGhdR0o5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "ed7d207d-d617-4668-8367-287c1f0803f1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       xcenter     ycenter       width      height  confidence  class  \\\n",
              "0  1587.685059  496.165466  143.758911  260.257538    0.824172     29   \n",
              "1  1053.790283  664.041016   94.993835   78.028870    0.748516     29   \n",
              "2  1585.460449  496.756775  138.394287  270.051270    0.541024     51   \n",
              "\n",
              "             name  image_id  \n",
              "0   Other plastic       171  \n",
              "1   Other plastic       171  \n",
              "2  Rope & strings       171  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29050d16-9a12-4df1-ac92-6156acb4e9d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>xcenter</th>\n",
              "      <th>ycenter</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>confidence</th>\n",
              "      <th>class</th>\n",
              "      <th>name</th>\n",
              "      <th>image_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1587.685059</td>\n",
              "      <td>496.165466</td>\n",
              "      <td>143.758911</td>\n",
              "      <td>260.257538</td>\n",
              "      <td>0.824172</td>\n",
              "      <td>29</td>\n",
              "      <td>Other plastic</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1053.790283</td>\n",
              "      <td>664.041016</td>\n",
              "      <td>94.993835</td>\n",
              "      <td>78.028870</td>\n",
              "      <td>0.748516</td>\n",
              "      <td>29</td>\n",
              "      <td>Other plastic</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1585.460449</td>\n",
              "      <td>496.756775</td>\n",
              "      <td>138.394287</td>\n",
              "      <td>270.051270</td>\n",
              "      <td>0.541024</td>\n",
              "      <td>51</td>\n",
              "      <td>Rope &amp; strings</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29050d16-9a12-4df1-ac92-6156acb4e9d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-29050d16-9a12-4df1-ac92-6156acb4e9d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-29050d16-9a12-4df1-ac92-6156acb4e9d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "truth_pd[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "qwJA0H4QrHYC",
        "outputId": "8aeb30b6-54ef-4ced-c3ce-369e5eaf4087"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   xcenter     ycenter  width      height  confidence  class  image_id\n",
              "0   1045.8  669.300384  110.0   81.000192           1     58       171\n",
              "1   1634.8  500.399808  240.0  268.000320           1     51       171"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a4ad431-dc1b-4ce6-805c-43e3b8d091bb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>xcenter</th>\n",
              "      <th>ycenter</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>confidence</th>\n",
              "      <th>class</th>\n",
              "      <th>image_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1045.8</td>\n",
              "      <td>669.300384</td>\n",
              "      <td>110.0</td>\n",
              "      <td>81.000192</td>\n",
              "      <td>1</td>\n",
              "      <td>58</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1634.8</td>\n",
              "      <td>500.399808</td>\n",
              "      <td>240.0</td>\n",
              "      <td>268.000320</td>\n",
              "      <td>1</td>\n",
              "      <td>51</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a4ad431-dc1b-4ce6-805c-43e3b8d091bb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a4ad431-dc1b-4ce6-805c-43e3b8d091bb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a4ad431-dc1b-4ce6-805c-43e3b8d091bb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Accuracy evaluation\n",
        "\n",
        "Usually, `Object Detection` tasks are measured by mAP, which is also the default metrics YoloV5 uses. You can also check Yolo's Precision and Recall metrics. \n",
        "\n",
        "However, if an `accuracy` metric is specifically needed, the following codes will do it.\n",
        "\n",
        "For each object with a truth bounding box in each image, if there is a prediction bounding box that has an IOU > threshold with that truth bounding box, it is counted as `detected`.\n",
        "\n",
        "For overall model `accuracy`, we count total number of `detected` of all images over total number of `predictions` of all images."
      ],
      "metadata": {
        "id": "TyiyfCEAODQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition of `Accuracy`:**\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
        "\n",
        "A comparison of `Accuracy, Precision, Recall`:\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}\n",
        "$$\n",
        "\n",
        "$$\\text{Precision} = \\frac{TP}{TP+FP}\n",
        "$$\n",
        "\n",
        "$$\\text{Recall} = \\frac{TP}{TP+FN}\n",
        "$$\n",
        "CC: https://developers.google.com/machine-learning/crash-course/classification/accuracy"
      ],
      "metadata": {
        "id": "Ff6hBppj1SjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bbox_iou(box1, box2, eps=1e-7):\n",
        "  \"\"\"\n",
        "  CITATION: adapted from YOLOV5 utils, author, cr: ultralytics\n",
        "  Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n",
        "  Get the coordinates of bounding boxes, transform from xywh to xyxy\n",
        "  \"\"\"\n",
        "  (x1, y1, w1, h1), (x2, y2, w2, h2) = box1.chunk(4, 1), box2.chunk(4, 1)\n",
        "  w1_, h1_, w2_, h2_ = w1 / 2, h1 / 2, w2 / 2, h2 / 2\n",
        "  b1_x1, b1_x2, b1_y1, b1_y2 = x1 - w1_, x1 + w1_, y1 - h1_, y1 + h1_\n",
        "  b2_x1, b2_x2, b2_y1, b2_y2 = x2 - w2_, x2 + w2_, y2 - h2_, y2 + h2_\n",
        "\n",
        "  inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
        "          (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
        "  union = w1 * h1 + w2 * h2 - inter + eps\n",
        "  return inter / union  # return IoU\n",
        "  \n",
        "def each_pic(pred_df,truth_df,iou_th,must_class):\n",
        "  \"\"\"\n",
        "  returns number of objects (truth) and number of detection\n",
        "  e.g. if there are 5 pieces of trash in an image and we predicted 2, it will return 5,2\n",
        "  \"\"\"\n",
        "  pred_df_ = pred_df.assign(matched=[0]*pred_df.shape[0])\n",
        "  nr_preds = pred_df.shape[0]\n",
        "  nr_dets = 0\n",
        "  for i in truth_df.iterrows():\n",
        "    tbox_tensor = torch.tensor([i[1].tolist()[:4]])\n",
        "    tlabel = i[1].tolist()[5]\n",
        "    \n",
        "    row_counter=0\n",
        "    for j in pred_df_.iterrows():\n",
        "      pbox_tensor = torch.tensor([j[1].tolist()[:4]])\n",
        "      plabel = j[1].tolist()[5]\n",
        "      matched = j[1].tolist()[-1]\n",
        "      if must_class==True: # if the detection has to assign a correct class name. \n",
        "        if bbox_iou(tbox_tensor,pbox_tensor)>iou_th and matched==0 and tlabel==plabel:\n",
        "          nr_dets+=1\n",
        "          pred_df_.iat[row_counter,-1]=1 # mark matched bbox, so one prediction bbox wont be counted as \"detected\" for two different objects\n",
        "          continue\n",
        "      else: \n",
        "        if bbox_iou(tbox_tensor,pbox_tensor)>iou_th and matched==0:\n",
        "          nr_dets+=1\n",
        "          pred_df_.iat[row_counter,-1]=1\n",
        "          continue\n",
        "      row_counter+=1\n",
        "  return nr_preds,nr_dets\n",
        "\n",
        "def get_accuracy(pred,truth,iou_th=0.5,must_class=False):\n",
        "  \"\"\"\n",
        "  pred: prediction list of dataframe\n",
        "  truth: truth list of dataframe\n",
        "  iou_th IOU threshold you define suitable\n",
        "  must_class: controls whether the category need to be predicted correctly\n",
        "              when set to false, only consider whether predicted bbox bounded objects correctly, \n",
        "              without considering if the correct class is identified\n",
        "  \"\"\"\n",
        "  preds,dets=0,0\n",
        "  for i in tqdm(range(len(truth))):\n",
        "    p,d=each_pic(pred_pd[i],truth_pd[i],iou_th,must_class)\n",
        "    preds+=p\n",
        "    dets+=d\n",
        "  return np.round(dets/preds,6)"
      ],
      "metadata": {
        "id": "LkQgO9T5OHzs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy**"
      ],
      "metadata": {
        "id": "t1nk9VlW3KZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = get_accuracy(pred_pd,truth_pd,iou_th=0.5,must_class=True)\n",
        "print('\\nOur trained model has an accuracy of: '+str(accuracy*100)+'%')"
      ],
      "metadata": {
        "id": "p8E9EsAMOH1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13057ba0-6a3f-4615-8f15-84726fc5a570"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 265.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Our trained model has an accuracy of: 37.5691%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pseudo-Accuracy**\n",
        "\n",
        "Some detection tasks care only about \"having a bounding box over the target object,\" they do not care about if the model label the object with a correct class. If you want such accuracy, it can be obtained by setting `must_class` to `False`. \n",
        "\n",
        "Below is an example:"
      ],
      "metadata": {
        "id": "fZWitSXb3EHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = get_accuracy(pred_pd,truth_pd,iou_th=0.5,must_class=False)\n",
        "print('\\nOur trained model has an accuracy of: '+str(accuracy*100)+'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28BGyq9g3Bwc",
        "outputId": "e264e872-f8aa-4426-8abc-b333a220c366"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 290.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Our trained model has an accuracy of: 77.3481%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}